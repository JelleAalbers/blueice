{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "matplotlib.rc('font', size=16)\n",
    "plt.rcParams['figure.figsize'] = (12.0, 10.0)    # resize plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up a log likelihood function for each wimp mass. We could also have set up one likelihood function with many wimps, it doesn't make much difference.\n",
    "\n",
    "(this approach is less memory-efficient as we have more pdfs in RAM, but a bit more efficient in CPU: having many WIMPs per model means you have to score each event for many sources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from wimpy.xenon.base_model import config as base_config\n",
    "from wimpy.xenon.base_model import nr_ignore_settings\n",
    "from wimpy.likelihood import LogLikelihood\n",
    "from scipy import stats\n",
    "\n",
    "lfs = {}\n",
    "wimp_masses = [10, 20, 50, 100, 1000]\n",
    "# Sample low-mass wimps more, since most of their events are out of range\n",
    "# Maybe we can cut off their spectra at 0.5 keV or something.\n",
    "# 6 GeV would require something like a 250x multiplier... too impatient\n",
    "sampling_multipliers = {10: 10, 20:2}   # Maybe 5x is overkill, but ok\n",
    "\n",
    "for m in wimp_masses:    \n",
    "    # Copy the base config and remove the wimps present there\n",
    "    config = deepcopy(base_config)\n",
    "    config['pdf_sampling_multiplier'] = 2\n",
    "    config['sources'] = [s for s in config['sources'] if not s['name'].startswith('wimp')]\n",
    "    \n",
    "    # Add this WIMP\n",
    "    config['sources'].append({\n",
    "        'energy_distribution': 'wimp_%dgev_1e-45.pkl' % m,   # TODO: generate and save on the fly\n",
    "        'color': 'red',\n",
    "        'recoil_type': 'nr',\n",
    "        'name': 'wimp_%dgev' % m,\n",
    "        'n_events_for_pdf': 5e6 * sampling_multipliers.get(m, 1),\n",
    "        'ignore_settings': nr_ignore_settings,\n",
    "        'label': '%d GeV WIMP' % m})\n",
    "    \n",
    "    # Create the log likelihood function\n",
    "    lf = LogLikelihood(config)\n",
    "    lf.add_rate_parameter('wimp_%dgev' % m)\n",
    "    lf.add_shape_parameter('leff', \n",
    "                           {t: 'leff_mcpaper_%s.csv' % t for t in [-2, -1, 0, 1, 2]}, \n",
    "                           log_prior=stats.norm.logpdf)\n",
    "    lf.prepare()\n",
    "    \n",
    "    # Store it in the dictionary\n",
    "    lfs[m] = lf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the function that will set a WIMP limit. We'll work with the \"wimp strength\" variable, defined as \n",
    "\n",
    "    log10(cross section / 1e-45 cm^2)\n",
    "    \n",
    "or equivalently\n",
    "\n",
    "    log10(event rate / rate for a wimp with 1e-45 cm^2 cross section)\n",
    "    \n",
    "Here `event_rate` meant the rate of *all* wimp interactions in the detector, not just the ones that produce events in our analysis range! The fraction of events in range depends on the wimp mass and Leff.\n",
    "\n",
    "It uses the `bestfit_scipy` function from wimpy.analysis, which simply makes our likelihood function talk to the scipy minimize. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import brentq\n",
    "from scipy import stats\n",
    "\n",
    "from wimpy.analysis import bestfit_scipy as bestfit\n",
    "\n",
    "def wimplimit(lf, profile=True, confidence_level=0.9, target=None, ndf=1, **kwargs):\n",
    "    \"\"\"Computes a wimp limit using the LogLikelihood function lf\n",
    "    profile: whether or not to profile over the other parameters of lf.\n",
    "    kwargs are passed to bestfit_scipy (and eventually to lf).\n",
    "    \"\"\"\n",
    "    if target is None:\n",
    "        target = lf.source_list[-1]\n",
    "    base_rate = lf.base_model.get_source(target).events_per_day\n",
    "        \n",
    "    # Find the likelihood of the global best fit (denominator of likelihood ratio)\n",
    "    target = '%s_rate' % target\n",
    "    result, max_loglikelihood = bestfit(lf, **kwargs)\n",
    "    # A strength of less -6 means we never see a wimp; the difference with -inf (or whatever it is) is insignificant\n",
    "    # Forgot exactly what problems you get into if you don't do this... but there were some\n",
    "    best_strength = max(-6, np.log10(result[target] / base_rate))\n",
    "    \n",
    "    # Find the best fit conditional to a wimp strength (numerator of likelihood ratio)\n",
    "    def f(wimp_strength, critical_chi2):\n",
    "        lf_kwargs = {target: base_rate * 10**wimp_strength}\n",
    "        lf_kwargs.update(kwargs)\n",
    "        if profile:\n",
    "            fitresult, ll= bestfit(lf, **lf_kwargs)\n",
    "        else:\n",
    "            ll = lf(**lf_kwargs)\n",
    "        result = np.sqrt(2*(max_loglikelihood - ll)) - critical_chi2\n",
    "        return result\n",
    "        \n",
    "    # Perform a line search \n",
    "    return brentq(f, best_strength, max(best_strength + 2, 2),\n",
    "                  args=(stats.chi2(ndf).ppf(0.9),))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load background data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I load a standard dataset of 1000 background samples, so I can study and refer to particular problematic ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bg_filename = 'bg_dsets.pkl'\n",
    "\n",
    "# n_trials = 1000\n",
    "# bg_dsets = [lfs[50].base_model.simulate(rate_multipliers=dict(wimp_50gev=0)) \n",
    "#             for _ in tqdm(range(n_trials))]\n",
    "# import pickle\n",
    "# with open(bg_filename, mode='wb') as outfile:\n",
    "#     pickle.dump(bg_dsets, outfile)\n",
    "\n",
    "with open(bg_filename, mode='rb') as infile:\n",
    "    bg_dsets = pickle.load(infile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is one background-only dataset. As it happens this contains a single neutron and no CNNS events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d = bg_dsets[0]\n",
    "lf = lfs[50]\n",
    "lf.base_model.show(d)\n",
    "plt.legend(loc='lower right', scatterpoints=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what the log likelihood function (for a 50 GeV WIMP) thinks of this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lf.set_data(d)\n",
    "plot_likelihood_ratio(lf, ('wimp_50gev_rate', np.logspace(-4, 1, 100)), ('leff', np.linspace(-2, 2, 100)))\n",
    "plt.xscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sigma_to_p(sigma):\n",
    "    return stats.norm.cdf(sigma) - stats.norm.cdf(-sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from wimpy.analysis import plot_likelihood_ratio\n",
    "\n",
    "base_rate = lf.base_model.get_source('wimp_50gev').events_per_day\n",
    "\n",
    "for leff in [2, 0, -2]:\n",
    "    plot_likelihood_ratio(lf, ('wimp_50gev_rate', np.logspace(-4, 0, 50)), leff=leff, \n",
    "                          plot_kwargs=dict(label=\"Leff t=%s\" % leff), vmax=5)\n",
    "    # strength = wimplimit(lf, leff=leff, profile=False)\n",
    "\n",
    "plt.axhline(stats.chi2(1).ppf(0.9)**2/2, linestyle='--', label='90% asymptotic', c='gray')\n",
    "plt.xscale('log')\n",
    "plt.legend(loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %%timeit\n",
    "# lfs[50](wimp_50_gev_rate=1, leff=0.1)\n",
    "# 0.5 ms / likelihood function call\n",
    "# Could probably do a small brute force search to seed the minimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bestfit(lf, guess=dict(wimp_50gev_rate=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bestfit(lf, guess=dict(wimp_50gev_rate=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "limits_lr = {m: [] for m in wimp_masses}\n",
    "limits_plr = {m: [] for m in wimp_masses}\n",
    "limits_lr_lowleff = {m: [] for m in wimp_masses}\n",
    "\n",
    "for m in wimp_masses:\n",
    "    lf = lfs[m]\n",
    "    \n",
    "    for i, d in enumerate(tqdm(bg_dsets)):\n",
    "        lf.set_data(d)\n",
    "        guess = {'wimp_%dgev_rate' % wimp_mass: 1e-2}\n",
    "        limits_lr[m].append(wimplimit(lf, profile=False, leff=0, guess=guess))\n",
    "        limits_lr_lowleff[m].append(wimplimit(lf, profile=False, leff=-2, guess=guess))\n",
    "        try:\n",
    "            limits_plr[m].append(wimplimit(lf, profile=True, guess=guess))\n",
    "        except Exception as e:\n",
    "            print(\"Failure in limit setting for dataset %d, %d GeV: %s, %s\" % (i, m, str(type(e)), str(e)))\n",
    "        \n",
    "    limits_lr[m] = np.array(limits_lr[m])\n",
    "    limits_plr[m] = np.array(limits_plr[m])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the Bologna model data points\n",
    "import pandas as pd\n",
    "ms, xs = pd.read_csv('./bologna_nocls_limits/median_nocls.csv', skiprows=5, names=['m', 'xs'])[1:].values.astype(np.float).T\n",
    "xs += 45\n",
    "plt.plot(ms, xs, color='k', linestyle=':', label='Bologna model (no CLS)', linewidth=2)\n",
    "\n",
    "# Plot the median of the limits we just derived, and the bands for the profiled limits\n",
    "for limit_dict, label, color in ((limits_lr, 'Leff = 0', 'g'),\n",
    "                                 (limits_lr_lowleff, 'Leff = -2', 'r'),\n",
    "                                 (limits_plr, 'Leff profiled', 'b')):\n",
    "    limits_sigma = {sigma: np.array([np.percentile(limit_dict[m], 100*stats.norm.cdf(sigma)) \n",
    "                                     for m in wimp_masses])\n",
    "                    for sigma in [-2, -1, 0, 1, 2]}\n",
    "    plt.plot(wimp_masses, limits_sigma[0], \n",
    "             marker='o', color=color, label=label)\n",
    "    if color == 'b':\n",
    "        plt.fill_between(wimp_masses, limits_sigma[-1], limits_sigma[1], alpha=0.1, color=color)\n",
    "        plt.fill_between(wimp_masses, limits_sigma[-2], limits_sigma[2], alpha=0.03, color=color)\n",
    "\n",
    "plt.xscale('log')\n",
    "plt.legend(loc='upper right', numpoints=1, frameon=False)\n",
    "plt.xlim(7, 1100)\n",
    "plt.ylim(-2.5, 0.5)\n",
    "plt.xlabel(\"Wimp mass (GeV)\")\n",
    "plt.ylabel(\"Log10 (cross section (zb))\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leff moves the NR band horizontally; the lower LEff, the further left it goes. This has two effects: it pushes events below threshold (the rate effect) and pushes them nearer to the ER band (the shape effect). Both weaken your sensitivity; the first obivously so, the second because it makes ER leakage events more likely to be interpreted as WIMP events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lf= lfs[1000]\n",
    "for i, ((leff,), m) in enumerate(sorted(lf.anchor_models.items())):\n",
    "    m.sources[-1].pdf_histogram.percentile(50, axis=1).plot(label='Nr median, Leff %d' % leff, color=plt.cm.Reds(i/4))\n",
    "   \n",
    "m.sources[0].pdf_histogram.plot(cblabel='ER PDF', log_scale=True, vmin=1e-9, cmap=plt.cm.viridis)\n",
    "#m.sources[0].pdf_histogram.percentile().plot(cblabel='ER PDF', log_scale=True, vmin=1e-8, cmap=plt.cm.Blues)\n",
    "leg = plt.legend(loc='upper left')\n",
    "leg.get_frame().set_facecolor('lightgray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularity of likelihood function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_likelihood_ratio(lf, ('leff', np.linspace(-2, 2, 100)), wimp_1000gev_rate=1e-3, vmax=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Check if pdf differences are well sampled\n",
    "lf= lfs[1000]\n",
    "s_i = -1\n",
    "a = lf.anchor_models[(0,)].sources[s_i]\n",
    "b = lf.anchor_models[(-1.0,)].sources[s_i]\n",
    "fr_increase = (b.pdf_histogram - a.pdf_histogram )/a.pdf_histogram\n",
    "#q = max(abs(fr_increase.histogram.max()), abs(fr_increase.histogram.min()))\n",
    "#q *= 0.5\n",
    "q = 1\n",
    "fr_increase.plot(vmin=-q, vmax=q, cmap=plt.cm.seismic, cblabel=\"Fractional increase in PDF\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
